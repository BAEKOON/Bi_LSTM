{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code by Tae Hwan Jung(Jeff Jung) @graykode\n",
    "# Reference : https://github.com/prakashpandey9/Text-Classification-Pytorch/blob/master/models/LSTM_Attn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Attention(nn.Module):#신경망모듈 기본 클래스\n",
    "    # ^ Pytorch Module내에 딥러닝 모델 관련 기본 함수를 포함하고 있는 nn.Module 클레스를 상속받는 Net클래스를 정의.\n",
    "    def __init__(self): \n",
    "        super(BiLSTM_Attention, self).__init__() #클래스 초기화(bound super object; requires isinstance(obj, type))\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) #사전의 크기, 임배딩 벡터의 크기\n",
    "        self.lstm = nn.LSTM(embedding_dim, n_hidden, bidirectional=True) #LSTM(임배딩 벡터의 크기, 은닉층 사이즈 ,양방향 LSTM)\n",
    "        self.out = nn.Linear(n_hidden * 2, num_classes)#선형변환(은닉유닛 사이즈*2[입력 샘플 크기], 출력 샘플 크기)\n",
    "\n",
    "    # v lstm_output : [batch_size, n_step, n_hidden * num_directions(=2)], F matrix\n",
    "    def attention_net(self, lstm_output, final_state): #어탠션 망 함수(self, lstm_output, 최종상태)\n",
    "        #print(\"lstm_output : \", lstm_output.size()) #6단 3행 10열\n",
    "        #print(\"final_state : \", final_state.size()) #2단 6행 5열\n",
    "        hidden = final_state.view(-1, n_hidden * 2, 1) \n",
    "        # 양식에 맞게 변환, hidden : [batch_size, n_hidden * num_directions(=2), 1(=n_layer)]\n",
    "        #print(\"hidden : \", hidden.size()) #6단 10행 1열\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) # 행렬 곱 수행, attn_weights : [batch_size, n_step]\n",
    "        #print(\"attn_weights : \", attn_weights.size()) #6행3열\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1) # 계산할 깊이가 2, (input(tensor), dim)\n",
    "        #print(\"softmax : \", soft_attn_weights) #6행3열\n",
    "        # [batch_size, n_hidden * num_directions(=2), n_step] * [batch_size, n_step, 1] = [batch_size, n_hidden * num_directions(=2), 1]\n",
    "        #print(lstm_output)\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        # transpose(차원 전치), 차원을 늘리고 3(unsqueeze), 차원을 줄임 3(squeeze)\n",
    "        return context, soft_attn_weights.data.numpy() # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "                                            #tensor를 numpy로 반환\n",
    "    def forward(self, X):\n",
    "        input = self.embedding(X) # input : [batch_size, len_seq, embedding_dim]\n",
    "        #print(\"input1 : \", input) # 6단 3행 2열\n",
    "        input = input.permute(1, 0, 2) # input : [len_seq, batch_size, embedding_dim](원하는 차원의 순서 변환, 1,0,2)\n",
    "        #print(\"input2 : \", input) #3단 6행 2열 \n",
    "        hidden_state = torch.zeros(1*2, len(X), n_hidden) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden], 2단 6행 5열\n",
    "        cell_state = torch.zeros(1*2, len(X), n_hidden) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "        #print(\"len(x) : \", len(X)) 결과:6\n",
    "        \n",
    "        # final_hidden_state, final_cell_state : [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (hidden_state, cell_state)) \n",
    "        output = output.permute(1, 0, 2) # output : [batch_size, len_seq, n_hidden]\n",
    "        attn_output, attention = self.attention_net(output, final_hidden_state)\n",
    "        return self.out(attn_output), attention # model : [batch_size, num_classes], attention : [batch_size, n_step]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 준비 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_dic :  {'likes': 0, 'hate': 1, 'you': 2, 'me': 3, 'love': 4, 'sorry': 5, 'football': 6, 'i': 7, 'awful': 8, 'he': 9, 'loves': 10, 'this': 11, 'she': 12, 'is': 13, 'for': 14, 'that': 15}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  # 프로그램의 시작점\n",
    "    embedding_dim = 2 # embedding size\n",
    "    n_hidden = 5  # number of hidden units in one cell\n",
    "    num_classes = 2  # 0 or 1\n",
    "\n",
    "    # 3 words sentences (=sequence_length is 3)\n",
    "    sentences = [\"i love you\", \"he loves me\", \"she likes football\", \"i hate you\", \"sorry for that\", \"this is awful\"]\n",
    "    labels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is not good.\n",
    "\n",
    "    word_list = \" \".join(sentences).split() # join = 리스트에서 문자열로 토큰화\n",
    "    word_list = list(set(word_list)) # set = 집합선언\n",
    "    word_dict = {w: i for i, w in enumerate(word_list)} # 딕셔너리 선언, 몇번째 반복인지 확인\n",
    "    print(\"word_dic : \", word_dict) # ex) i:0, love:2....15\n",
    "    vocab_size = len(word_dict)  # dict의 길이\n",
    "\n",
    "    model = BiLSTM_Attention()   #모델 지정\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    ''' \n",
    "    ^ loss 계산, corssentropy = C 클래스 로 분류 문제를 훈련 할 때 유용합니다 . \n",
    "    제공되는 경우 선택적 인수 weight는 각 클래스에 가중치를 할당 하는 1D Tensor 여야합니다 .\n",
    "    \n",
    "    이 모델의 output값과 계산될 Label값은 Class를 표현하는 원-핫 인코딩 값임.\n",
    "    모델의 output한 값과 원-핫 인코딩 결과의 Loss는 CrossEntropy를 이용하여 계산하기 위함.\n",
    "    '''\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001) # 매개변수 최적화(model.parameters()), 아담\n",
    "    '''\n",
    "    ^ Back propagation을 통해 파라미터를 업데이트 할때 이용하는 Optimizer를 정의\n",
    "    '''\n",
    "    inputs = torch.LongTensor([np.asarray([word_dict[n] for n in sen.split()]) for sen in sentences])\n",
    "    #print(\"inputs : \", inputs.size()) #6행 3열\n",
    "    # ^ asarray는 ndarray(다차원 행렬 구조)로 타입 변경, ndarray는 기존 파이썬과는 다르게 오직 같은 종류의 데이터만을 배열에 담을 수 있다.\n",
    "    targets = torch.LongTensor([out for out in labels])  # To using Torch Softmax Loss function\n",
    "    #print(targets) 1행 6열\n",
    "    #텐서에는 자료형이라는 것이 있습니다. 각 데이터형별로 정의되어져 있는데, 64비트의 부호 있는 정수는 torch.LongTensor를 사용합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 cost = 0.002880\n",
      "Epoch: 2000 cost = 0.000591\n",
      "Epoch: 3000 cost = 0.000231\n",
      "Epoch: 4000 cost = 0.000111\n",
      "Epoch: 5000 cost = 0.000059\n"
     ]
    }
   ],
   "source": [
    "    for epoch in range(5000):\n",
    "        optimizer.zero_grad()  # 최적화된 그레디언트 값을 0으로 반환\n",
    "        ''' \n",
    "        ^ 기존에 정의한 장비에 단어 데이터와 레이블 데이터를 할당할 경우, \n",
    "        과거에 이용한 mini-batch내에 있는 이미지 데이터와 레이블 데이터를 바탕으로 계산된 Loss의 Gradient값이 Optimizer에 할당돼 있으므로 \n",
    "        optimizer의 Gradient를 초기화 함.\n",
    "        '''\n",
    "        output, attention = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        \n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "        loss.backward()   # 역전파\n",
    "        ''' \n",
    "        ^ 계산된 loss 값에 대해 backward() 매서드를 이용하면 각 파라미터 값에 대해 Gradient를 계산하고\n",
    "        이를 통해 Back Progpagation을 진행한다는 것을 의미\n",
    "        '''\n",
    "        optimizer.step()   # 단일 최적화 단계 수행    \n",
    "        '''\n",
    "        ^ Loss값을 계산한 결과를 바탕으로 각 파라미터에 할당된 Gradient값을 이용해 파라미터 값을 업데이트.\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorry hate you is Bad Mean...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: FixedFormatter should only be used together with FixedLocator\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# v Test 자료 준비 \n",
    "    test_text = 'sorry hate you'\n",
    "    #test_text='sorry loves you'\n",
    "    tests = [np.asarray([word_dict[n] for n in test_text.split()])]\n",
    "    #print(\"tests : \", tests) #[array([12,10,2])]\n",
    "    test_batch = torch.LongTensor(tests)\n",
    "    #print(\"test_batch :\", test_batch.size()) #tensor([1,3])\n",
    "\n",
    "# Predict\n",
    "    predict, _ = model(test_batch)\n",
    "    predict = predict.data.max(1, keepdim=True)[1] # 텐서의 최대값 반환, 1차원으로 줄여야할 차원이 있는가?\n",
    "    if predict[0][0] == 0:\n",
    "        print(test_text,\"is Bad Mean...\")\n",
    "    else:\n",
    "        print(test_text,\"is Good Mean!!\")\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 3)) # [batch_size, n_step]\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    ax.set_xticklabels(['']+['first_word', 'second_word', 'third_word'], fontdict={'fontsize': 14}, rotation=90)\n",
    "    ax.set_yticklabels(['']+['batch_1', 'batch_2', 'batch_3', 'batch_4', 'batch_5', 'batch_6'], fontdict={'fontsize': 14})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
